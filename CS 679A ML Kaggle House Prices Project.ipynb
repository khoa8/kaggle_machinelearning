{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9595151831720452\n",
      "17454.534285430687\n",
      "\n",
      "0.9413594970468038\n",
      "21006.85412219963\n",
      "\n",
      "0.9145748668998716\n",
      "25354.497539593955\n",
      "\n",
      "0.9473022164401961\n",
      "19913.99304026523\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_filename = \"train.csv\"\n",
    "testing_filename = \"test.csv\"\n",
    "\n",
    "training_dataset = pd.read_csv(training_filename)\n",
    "testing_dataset = pd.read_csv(testing_filename)\n",
    "\n",
    "#plt.figure(figsize=(20, 20))\n",
    "#sns.heatmap(training_dataset.corr(), cmap='rocket', annot=True, fmt=f'0.1', cbar=False);\n",
    "\n",
    "# Handle Ordinal Categorial Features \n",
    "\n",
    "# Find out which features are more than 80% empty \n",
    "'''\n",
    "nullVal = pd.DataFrame({'Amount': training_dataset.isnull().sum(),\n",
    "              'Percent': (training_dataset.isnull().sum() / len(training_dataset)) *100}).sort_values(by='Percent', ascending=False)\n",
    "print(nullVal)\n",
    "\n",
    "print()\n",
    "\n",
    "nullValTest = pd.DataFrame({'Amount': testing_dataset.isnull().sum(),\n",
    "              'Percent': (testing_dataset.isnull().sum() / len(testing_dataset)) *100}).sort_values(by='Percent', ascending=False)\n",
    "print(nullValTest)'''\n",
    "\n",
    "training_dataset.pop('MiscFeature')\n",
    "training_dataset.pop('MoSold')\n",
    "training_dataset.pop('YrSold')\n",
    "\n",
    "testing_dataset.pop('MiscFeature')\n",
    "testing_dataset.pop('MoSold')\n",
    "testing_dataset.pop('YrSold')\n",
    "\n",
    "ordinal_feature_list1 = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', \n",
    "                         'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\n",
    "ordinal_feature1_mapping = {'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}\n",
    "for i in range(len(ordinal_feature_list1)):\n",
    "    training_dataset[ordinal_feature_list1[i]] = training_dataset[ordinal_feature_list1[i]].map(ordinal_feature1_mapping)\n",
    "    testing_dataset[ordinal_feature_list1[i]] = testing_dataset[ordinal_feature_list1[i]].map(ordinal_feature1_mapping)\n",
    "    \n",
    "ordinal_feature2_mapping = {'No':1, 'Mn':2, 'Av':3, 'Gd':4}\n",
    "training_dataset['BsmtExposure'] = training_dataset['BsmtExposure'].map(ordinal_feature2_mapping)\n",
    "testing_dataset['BsmtExposure'] = testing_dataset['BsmtExposure'].map(ordinal_feature2_mapping)\n",
    "\n",
    "ordinal_feature3_mapping = {'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6}\n",
    "training_dataset['BsmtFinType1'] = training_dataset['BsmtFinType1'].map(ordinal_feature3_mapping)\n",
    "training_dataset['BsmtFinType2'] = training_dataset['BsmtFinType2'].map(ordinal_feature3_mapping)\n",
    "testing_dataset['BsmtFinType1'] = testing_dataset['BsmtFinType1'].map(ordinal_feature3_mapping)\n",
    "testing_dataset['BsmtFinType2'] = testing_dataset['BsmtFinType2'].map(ordinal_feature3_mapping)\n",
    "\n",
    "ordinal_feature4_mapping = {'Unf':1, 'RFn':2, 'Fin':3}\n",
    "training_dataset['GarageFinish'] = training_dataset['GarageFinish'].map(ordinal_feature4_mapping)\n",
    "testing_dataset['GarageFinish'] = testing_dataset['GarageFinish'].map(ordinal_feature4_mapping)\n",
    "\n",
    "ordinal_feature5_mapping = {'MnWw':1, 'GdWo':2, 'MnPrv':3, 'GdPrv':4}\n",
    "training_dataset['Fence'] = training_dataset['Fence'].map(ordinal_feature5_mapping)\n",
    "testing_dataset['Fence'] = testing_dataset['Fence'].map(ordinal_feature5_mapping)\n",
    "\n",
    "\n",
    "# Handle Nominal Categorial Features\n",
    "training_dataset = training_dataset.fillna(0)\n",
    "testing_dataset = testing_dataset.fillna(0)\n",
    "\n",
    "train_col_list = list(training_dataset.select_dtypes(include=['object']).columns)\n",
    "test_col_list = list(testing_dataset.select_dtypes(include=['object']).columns)\n",
    "\n",
    "for i in range(len(train_col_list)):\n",
    "    training_dataset = pd.get_dummies(training_dataset, columns=[train_col_list[i]])\n",
    "    \n",
    "for j in range(len(test_col_list)):\n",
    "     testing_dataset = pd.get_dummies(testing_dataset, columns=[test_col_list[j]])\n",
    "        \n",
    "# Check for outlying features\n",
    "\n",
    "Ids = training_dataset.pop('Id')\n",
    "testIds = testing_dataset.pop('Id')\n",
    "\n",
    "trainCol = set(training_dataset.columns)\n",
    "testCol = set(testing_dataset.columns)\n",
    "trainCol.remove('SalePrice')\n",
    "\n",
    "# Outlying features in training set that are not in testing \n",
    "trainList = list(trainCol - testCol)\n",
    "# Outlying features in testing set that are not in training \n",
    "testList = list(testCol - trainCol)\n",
    "\n",
    "'''\n",
    "for i in range(len(trainList)):\n",
    "    # Get the correlation of that feature with SalePrice to see if there is any significant relationship\n",
    "    print(training_dataset[trainList[i]].corr(training_dataset['SalePrice']), \"\\n\")\n",
    "    # Check how spread the values for the feature are\n",
    "    print(training_dataset[trainList[i]].value_counts(), \"\\n\")\n",
    "    print(training_dataset[trainList[i]].describe(), \"\\n\")\n",
    "\n",
    "print(\"\\n\\n\\n\\nTesting\\n\")\n",
    "\n",
    "for i in range(len(testList)):\n",
    "    # Check how spread the valyes for the feature are\n",
    "    print(testing_dataset[testList[i]].value_counts())\n",
    "    print(testing_dataset[testList[i]].describe())\n",
    "'''\n",
    "    \n",
    "# Since all the features have very weak correlations to SalePrice and their value_counts indicate in every feature\n",
    "# there are at most less than 10/1460 that have different values than the rest. This is enough to indicate these \n",
    "# features are outlying features and can be excluded from the model.\n",
    "\n",
    "# Drop the outlying features\n",
    "for i in range(len(trainList)):\n",
    "    training_dataset.pop(trainList[i])\n",
    "    \n",
    "for i in range(len(testList)):\n",
    "    testing_dataset.pop(testList[i])\n",
    "    \n",
    "salePrice = training_dataset.pop('SalePrice')\n",
    "\n",
    "    \n",
    "# More Data Preprocessing   \n",
    "\n",
    "\n",
    "mincorrList = {}\n",
    "smallCorrList = {}\n",
    "medCorrList = {}\n",
    "highCorrList = {}\n",
    "\n",
    "for elem in training_dataset.columns:\n",
    "    if abs(training_dataset[elem].corr(salePrice)) < 0.1:\n",
    "        mincorrList[elem] = training_dataset[elem].corr(salePrice)\n",
    "    elif abs(training_dataset[elem].corr(salePrice)) >= 0.1 and abs(training_dataset[elem].corr(salePrice)) <= 0.4:\n",
    "        smallCorrList[elem] = training_dataset[elem].corr(salePrice)\n",
    "    elif abs(training_dataset[elem].corr(salePrice)) > 0.4 and abs(training_dataset[elem].corr(salePrice)) <= 0.7:\n",
    "        medCorrList[elem] = training_dataset[elem].corr(salePrice)\n",
    "    elif abs(training_dataset[elem].corr(salePrice)) > 0.7 and abs(training_dataset[elem].corr(salePrice)) <= 1.0:\n",
    "        highCorrList[elem] = training_dataset[elem].corr(salePrice)\n",
    "\n",
    "'''\n",
    "#print(\"Little to no correlation:\")\n",
    "#for x,y in mincorrList.items():\n",
    "    #print(x)\n",
    "    #print(x,y)\n",
    "    #print(\"Length =\", len(mincorrList))\n",
    "    \n",
    "print(\"\\n\\nSmall correlation:\")\n",
    "for x,y in smallCorrList.items():\n",
    "    print(x,y)\n",
    "    print(\"Length =\", len(smallCorrList))\n",
    "\n",
    "print(\"\\n\\nMedium correlation:\")\n",
    "for x,y in medCorrList.items():\n",
    "    print(x,y)\n",
    "    print(\"Length =\", len(medCorrList))\n",
    "    \n",
    "print(\"\\n\\nHigh correlation:\")\n",
    "for x,y in highCorrList.items():\n",
    "    print(x,y)\n",
    "    print(\"Length =\", len(highCorrList))'''\n",
    "    \n",
    "'''lowCorrToRemove = mincorrList.keys()\n",
    "for elem in lowCorrToRemove:\n",
    "    training_dataset.pop(elem)\n",
    "    testing_dataset.pop(elem)'''\n",
    "########### Train and Predict ###########\n",
    "\n",
    "train, test, trainSalePrice, testSalePrice = train_test_split(training_dataset, salePrice, test_size=0.1, random_state=1)\n",
    "\n",
    "# XGB Model\n",
    "xgbModel = xgb.XGBRegressor(min_child_weight=3, learning_rate=0.1, colsample_bytree=0.2, subsample=0.7, n_estimators=500, reg_lambda=2, reg_alpha=0.5, max_depth=3, gamma=1, random_state=1)\n",
    "xgbModel = xgbModel.fit(train, trainSalePrice)\n",
    "predicted = xgbModel.predict(test)\n",
    "print(r2_score(testSalePrice, predicted))\n",
    "print(mean_squared_error(testSalePrice, predicted, squared=False))\n",
    "#testxgbPred = xgbModel.predict(testing_dataset)\n",
    "#xgboutput = pd.DataFrame({'Id':testIds,\n",
    "                          #'SalePrice':testxgbPred})\n",
    "#xgboutput.to_csv('submission.csv', index=False)\n",
    "print()\n",
    "\n",
    "# Random Forest Model\n",
    "rfModel = RandomForestRegressor(n_estimators=500, max_depth=10, random_state=1)\n",
    "rfModel = rfModel.fit(train, trainSalePrice)\n",
    "rfPredicted = rfModel.predict(test)\n",
    "print(r2_score(testSalePrice, rfPredicted))\n",
    "print(mean_squared_error(testSalePrice, rfPredicted, squared=False))\n",
    "'''testRFPred = rfModel.predict(testing_dataset)\n",
    "rfoutput = pd.DataFrame({'Id':testIds,\n",
    "                          'SalePrice':testRFPred})\n",
    "rfoutput.to_csv('submission.csv', index=False)'''\n",
    "print()\n",
    "\n",
    "# Gradient Boosting Model\n",
    "gbModel = GradientBoostingRegressor(loss='huber', learning_rate=0.1, n_estimators=500, max_depth=3, alpha=0.8, random_state=1)\n",
    "gbModel = gbModel.fit(train, trainSalePrice)\n",
    "gbPredicted = gbModel.predict(test)\n",
    "print(r2_score(testSalePrice, gbPredicted))\n",
    "print(mean_squared_error(testSalePrice, gbPredicted, squared=False))\n",
    "'''testgbPred = gbModel.predict(testing_dataset)\n",
    "gboutput = pd.DataFrame({'Id':testIds,\n",
    "                          'SalePrice':testgbPred})\n",
    "gboutput.to_csv('submission.csv', index=False)'''\n",
    "print()\n",
    "\n",
    "# LGB Model\n",
    "lgbModel = lgb.LGBMRegressor(max_depth=3, learning_rate=0.1, n_estimators=1000, subsample_for_bin=600, colsample_bytree=0.5, reg_alpha=0.5, random_state=1)\n",
    "lgbModel = lgbModel.fit(train, trainSalePrice)\n",
    "lgbPredicted = lgbModel.predict(test)\n",
    "print(r2_score(testSalePrice, lgbPredicted))\n",
    "print(mean_squared_error(testSalePrice, lgbPredicted, squared=False))\n",
    "#testlgbPred = lgbModel.predict(testing_dataset)\n",
    "#lgboutput = pd.DataFrame({'Id':testIds,\n",
    "                          #'SalePrice':testlgbPred})\n",
    "#lgboutput.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9593198189064971\n",
      "17496.598059012293\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "lgbModel = lgb.LGBMRegressor(max_depth=3, learning_rate=0.1, n_estimators=1000, subsample_for_bin=600, colsample_bytree=0.5, reg_alpha=0.5, random_state=1)\n",
    "xgbModel = xgb.XGBRegressor(min_child_weight=3, learning_rate=0.1, colsample_bytree=0.2, subsample=0.7, n_estimators=500, reg_lambda=2, reg_alpha=0.5, max_depth=3, gamma=1, random_state=1)\n",
    "rfModel = RandomForestRegressor(n_estimators=500, max_depth=10, random_state=1)\n",
    "gbModel = GradientBoostingRegressor(loss='huber', learning_rate=0.1, n_estimators=500, max_depth=3, alpha=0.8, random_state=1)\n",
    "reg3 = LinearRegression()\n",
    "\n",
    "ereg = VotingRegressor(estimators=[('lgb', lgbModel), ('xg', xgbModel)])\n",
    "ereg = ereg.fit(train, trainSalePrice)\n",
    "newpred = ereg.predict(test)\n",
    "print(r2_score(testSalePrice, newpred))\n",
    "print(mean_squared_error(testSalePrice, newpred, squared=False))\n",
    "testt = ereg.predict(testing_dataset)\n",
    "outputt = pd.DataFrame({'Id':testIds,'SalePrice':testt})\n",
    "outputt.to_csv('submission30.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:21:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:21:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:21:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:21:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:21:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:21:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:21:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:21:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:21:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:21:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "   train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0    179433.481250      828.774624   179339.990625    7372.454172\n",
      "1    163142.159375      784.706628   163120.540625    7092.911730\n",
      "2    148648.173437      802.675543   148569.840625    6776.880747\n",
      "3    135473.426563      813.773833   135561.844531    6679.675213\n",
      "4    123659.233594      900.761895   123510.743750    6357.390998\n",
      "45    31333.743359\n",
      "46    31158.060742\n",
      "47    31215.965234\n",
      "48    31111.370703\n",
      "49    30966.108594\n",
      "Name: test-rmse-mean, dtype: float64\n",
      "0.9150727051939116\n",
      "25280.509456085525\n"
     ]
    }
   ],
   "source": [
    "from xgboost import cv\n",
    "\n",
    "params1 = {\"objective\":'binary:logistic','colsample_bytree': 0.3,'learning_rate': 0.1,'max_depth': 5, 'alpha': 10}\n",
    "params2 = {\"min_child_weight\":3, 'learning_rate':0.1, 'colsample_bytree':0.2, 'subsample':0.7, 'n_estimators':500, 'reg_lambda':2, 'reg_alpha':0.5, 'max_depth':3, 'gamma':1, 'random_state':1}\n",
    "\n",
    "data_dmatrix = xgb.DMatrix(data=train,label=trainSalePrice)\n",
    "\n",
    "cv_results = cv(dtrain=data_dmatrix, params=params2, nfold=10, num_boost_round=50, early_stopping_rounds=10, metrics='rmse', as_pandas=True, seed=0)\n",
    "\n",
    "print(cv_results.head())\n",
    "print((cv_results[\"test-rmse-mean\"]).tail())\n",
    "\n",
    "xgbModel = xgbModel.fit(train, trainSalePrice)\n",
    "predicted = xgbModel.predict(test)\n",
    "print(r2_score(testSalePrice, predicted))\n",
    "print(mean_squared_error(testSalePrice, predicted, squared=False))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
