{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "-2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x1=[1,-1,1,-1,1,1]\n",
    "x2=[-1,1,-1,-1,1,-1]\n",
    "x3=[1,1,-1,1,1,1]\n",
    "\n",
    "r=[1,1,1,-1,-1,-1]\n",
    "print(np.dot(x1,r))\n",
    "print(np.dot(x2,r))\n",
    "print(np.dot(x3,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[3. 4.]\n",
      "[[-2. -2.]\n",
      " [ 0.  0.]\n",
      " [ 2.  2.]]\n",
      "[[4. 4.]\n",
      " [4. 4.]]\n",
      "[[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "[8. 0.]\n",
      "[[-2.82842712  0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 2.82842712  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "\n",
    "from numpy import array\n",
    "from numpy import mean\n",
    "from numpy import cov\n",
    "from numpy.linalg import eig\n",
    "# define a matrix\n",
    "A = array([[1, 2], [3, 4], [5, 6]])\n",
    "print(A)\n",
    "# calculate the mean of each column\n",
    "M = mean(A.T, axis=1)\n",
    "print(M)\n",
    "# center columns by subtracting column means\n",
    "C = A - M\n",
    "print(C)\n",
    "# calculate covariance matrix of centered matrix\n",
    "V = cov(C.T)\n",
    "print(V)\n",
    "# eigendecomposition of covariance matrix\n",
    "values, vectors = eig(V)\n",
    "print(vectors)\n",
    "print(values)\n",
    "# project data\n",
    "P = vectors.T.dot(C.T)\n",
    "print(P.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2[1.5pts]:Repeat Q1 using XGBoost and LightGBM algorithms.Experiment with at least 5 different parametersettings to see their effect on training and test errors.How do best parameters change as the #training instances change? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.29  0.37  1.    2.  ]\n",
      " [-0.2   0.3  -2.    1.  ]]\n",
      "[3.  0.2 0.5 1. ]\n",
      "[ 0.3 -0.7  1.   0.5]\n",
      "[1.616 1.23 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "# define a matrix\n",
    "\n",
    "vector= np.array([[0.29, 0.37, 1,2],[-0.2, 0.3, -2,1]])\n",
    "\n",
    "x= np.array([3, 0.2, 0.5, 1.0])\n",
    "\n",
    "mean= np.array([0.3, -0.7, 1.0, 0.5])\n",
    "print(vector)\n",
    "print(x)\n",
    "print(mean)\n",
    "\n",
    "# project data\n",
    "P = vector.dot(x-mean)\n",
    "print(P.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 1\n",
      "X25 Test Error=  0.084\n",
      "X25 Train Error=  0.0\n",
      "X100 Test Error=  0.052\n",
      "X100 Train Error=  0.0\n",
      "\n",
      "model: 2\n",
      "X25 Test Error=  0.084\n",
      "X25 Train Error=  0.0\n",
      "X100 Test Error=  0.061\n",
      "X100 Train Error=  0.0\n",
      "\n",
      "model: 3\n",
      "X25 Test Error=  0.108\n",
      "X25 Train Error=  0.0\n",
      "X100 Test Error=  0.061\n",
      "X100 Train Error=  0.0\n",
      "\n",
      "model: 4\n",
      "X25 Test Error=  0.133\n",
      "X25 Train Error=  0.012\n",
      "X100 Test Error=  0.055\n",
      "X100 Train Error=  0.003\n",
      "\n",
      "model: 5\n",
      "X25 Test Error=  0.12\n",
      "X25 Train Error=  0.0\n",
      "X100 Test Error=  0.064\n",
      "X100 Train Error=  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for i in range(5):\n",
    "    print('model:', i+1)\n",
    "    if i==0: \n",
    "        xg1=xgb.XGBClassifier(max_depth=5,learning_rate=.5)\n",
    "        xg2=xgb.XGBClassifier(max_depth=5,learning_rate=.5)\n",
    "    if i==1: \n",
    "        xg1=xgb.XGBClassifier(min_child_weight=1,gamma=0,max_depth=7,eval_metric='auc',learning_rate=.5)\n",
    "        xg2=xgb.XGBClassifier(min_child_weight=1,gamma=0,max_depth=7,eval_metric='auc',learning_rate=.5)\n",
    "    if i==2: \n",
    "        xg1=xgb.XGBClassifier(max_depth=7,objective='binary:logistic',eval_metric='auc',learning_rate=.1)\n",
    "        xg2=xgb.XGBClassifier(max_depth=7,objective='binary:logistic',eval_metric='auc',learning_rate=.1)\n",
    "    if i==3: \n",
    "        xg1=xgb.XGBClassifier(min_child_weight=5,max_depth=7,objective='binary:logistic',eval_metric='mae',learning_rate=.05)\n",
    "        xg2=xgb.XGBClassifier(min_child_weight=5,max_depth=7,objective='binary:logistic',eval_metric='mae',learning_rate=.05)\n",
    "    if i==4: \n",
    "        xg1=xgb.XGBClassifier(max_depth=9,objective='binary:logistic',eta=1,eval_metric='rmse',learning_rate=.05)\n",
    "        xg2=xgb.XGBClassifier(max_depth=9,objective='binary:logistic',eta=1,eval_metric='rmse',learning_rate=.05)\n",
    "    xg1.fit(x25_train,y25_train)\n",
    "#     if i==0:\n",
    "#         ypred=xg1.predict(x25_train)\n",
    "#         print(\"Accuracy11111:\",accuracy_score(ypred,y25_train))\n",
    "    print('X25 Test Error= ', round(1-xg1.score(x25_test,y25_test),3))\n",
    "    print('X25 Train Error= ', round(1-xg1.score(x25_train,y25_train),3))\n",
    "    xg2.fit(x100_train,y100_train) \n",
    "    print('X100 Test Error= ', round(1-xg2.score(x100_test,y100_test),3))\n",
    "    print('X100 Train Error= ', round(1-xg2.score(x100_train,y100_train),3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 1\n",
      "X25 Test Error=  0.759\n",
      "X25 Train Error=  0.737\n",
      "X100 Test Error=  0.609\n",
      "X100 Train Error=  0.549\n",
      "\n",
      "model: 2\n",
      "X25 Test Error=  0.145\n",
      "X25 Train Error=  0.0\n",
      "X100 Test Error=  0.639\n",
      "X100 Train Error=  0.622\n",
      "\n",
      "model: 3\n",
      "X25 Test Error=  0.133\n",
      "X25 Train Error=  0.024\n",
      "X100 Test Error=  0.115\n",
      "X100 Train Error=  0.01\n",
      "\n",
      "model: 4\n",
      "X25 Test Error=  0.096\n",
      "X25 Train Error=  0.0\n",
      "X100 Test Error=  0.058\n",
      "X100 Train Error=  0.0\n",
      "\n",
      "model: 5\n",
      "X25 Test Error=  0.096\n",
      "X25 Train Error=  0.0\n",
      "X100 Test Error=  0.064\n",
      "X100 Train Error=  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb \n",
    "\n",
    "for i in range(5):\n",
    "    print('model:', i+1)\n",
    "    if i==0: \n",
    "        lg1=lgb.LGBMClassifier(num_leaves=50, max_depth=7,learning_rate=1)\n",
    "        lg2=lgb.LGBMClassifier(num_leaves=50, max_depth=7,learning_rate=1)\n",
    "    if i==1: \n",
    "        lg1=lgb.LGBMClassifier(num_leaves=50, max_depth=7,learning_rate=.9)\n",
    "        lg2=lgb.LGBMClassifier(num_leaves=50, max_depth=7,learning_rate=.9)\n",
    "    if i==2: \n",
    "        lg1=lgb.LGBMClassifier(num_leaves=200, objective='binary',max_depth=7,learning_rate=.01,max_bin=20)\n",
    "        lg2=lgb.LGBMClassifier(num_leaves=200, objective='binary',max_depth=7,learning_rate=.01,max_bin=20)\n",
    "    if i==3: \n",
    "        lg1=lgb.LGBMClassifier(num_leaves=20, objective='binary',max_depth=7,learning_rate=.05,max_bin=20)\n",
    "        lg2=lgb.LGBMClassifier(num_leaves=20, objective='binary',max_depth=7,learning_rate=.05,max_bin=20)\n",
    "    if i==4: \n",
    "        lg1=lgb.LGBMClassifier(num_leaves=500, objective='binary',max_depth=7,learning_rate=.5,max_bin=250)\n",
    "        lg2=lgb.LGBMClassifier(num_leaves=500, objective='binary',max_depth=7,learning_rate=.5,max_bin=250)\n",
    "\n",
    "    lg1.fit(x25_train,y25_train) \n",
    "    print('X25 Test Error= ', round(1-lg1.score(x25_test,y25_test),3))\n",
    "    print('X25 Train Error= ', round(1-lg1.score(x25_train,y25_train),3))\n",
    "    lg2.fit(x100_train,y100_train) \n",
    "    print('X100 Test Error= ', round(1-lg2.score(x100_test,y100_test),3))\n",
    "    print('X100 Train Error= ', round(1-lg2.score(x100_train,y100_train),3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3[1.5pts]:Nonparametric Classification: Use sklearn.treelibraryâ€™s KneighborsClassifieralgorithm. For the KneighborsClassifierdetermine the value of the best kparameter (experiment with k=1, 3, 5,9) that results in the best test error for each of thetraining data sets you created. How does the best kvalue change as the number of instances change? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 1\n",
      "X25 Test Error=  0.072\n",
      "X25 Train Error=  0.0\n",
      "X100 Test Error=  0.03\n",
      "X100 Train Error=  0.0\n",
      "\n",
      "K: 3\n",
      "X25 Test Error=  0.072\n",
      "X25 Train Error=  0.036\n",
      "X100 Test Error=  0.03\n",
      "X100 Train Error=  0.015\n",
      "\n",
      "K: 5\n",
      "X25 Test Error=  0.096\n",
      "X25 Train Error=  0.048\n",
      "X100 Test Error=  0.03\n",
      "X100 Train Error=  0.022\n",
      "\n",
      "K: 9\n",
      "X25 Test Error=  0.096\n",
      "X25 Train Error=  0.054\n",
      "X100 Test Error=  0.042\n",
      "X100 Train Error=  0.033\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "for i in 1,3,5,9:\n",
    "    knn1 = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn2 = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn1.fit(x25_train ,y25_train)\n",
    "    knn2.fit(x100_train ,y100_train)\n",
    "#     ypred = knn1.predict(x25_test)\n",
    "#     print(accuracy_score(ypred,y25_test))\n",
    "    print('K:', i)\n",
    "    print('X25 Test Error= ', round(1-knn1.score(x25_test,y25_test),3))\n",
    "    print('X25 Train Error= ', round(1-knn1.score(x25_train,y25_train),3))\n",
    "    print('X100 Test Error= ', round(1-knn2.score(x100_test,y100_test),3))\n",
    "    print('X100 Train Error= ', round(1-knn2.score(x100_train,y100_train),3))\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4[3pts]:Decision Trees, regression for digit completion: Using only the datafor class 6 and class 9in X100for training,  use the first 48features as inputs and predict the next 16 features, i.e. create 16 decision tree regression models, using the sklearn library. Report the test error(use only the instances from classes 6 and 9)for each of the 16 regression models. Which pixels are easier to predict?(Clarification, each of your models will have the same set of features, namely features 1...48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 65)\n",
      "(200, 65)\n",
      "\n",
      "Test Error:\n",
      "Feature  48 : 0.0\n",
      "Feature  49 : 0.288\n",
      "Feature  50 : 0.652\n",
      "Feature  51 : 0.758\n",
      "Feature  52 : 0.879\n",
      "Feature  53 : 0.788\n",
      "Feature  54 : 0.758\n",
      "Feature  55 : 0.303\n",
      "Feature  56 : 0.0\n",
      "Feature  57 : 0.076\n",
      "Feature  58 : 0.636\n",
      "Feature  59 : 0.818\n",
      "Feature  60 : 0.818\n",
      "Feature  61 : 0.773\n",
      "Feature  62 : 0.667\n",
      "Feature  63 : 0.136\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "print(X100.shape)\n",
    "X100_69 = pd.concat([X100.loc[X100[64] == 6],X100.loc[X100[64] == 9]]).sort_index()\n",
    "print(X100_69.shape)\n",
    "# X100_69\n",
    "print()\n",
    "print('Test Error:')\n",
    "for i in range(16):\n",
    "    \n",
    "    xtra, xtes, ytra, ytes = train_test_split(X100_69.iloc[:,0:48], X100_69.iloc[:,48+i], test_size=0.33)\n",
    "    reg=DecisionTreeRegressor()\n",
    "    reg.fit(xtra,ytra)\n",
    "    ypred=reg.predict(xtes)\n",
    "    print('Feature ',48+i,':',round(1-accuracy_score(ypred,ytes),3))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
